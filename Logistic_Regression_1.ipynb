{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "65dc9e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "0382bb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563, 32)"
      ]
     },
     "execution_count": 768,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp=pd.read_csv('data.csv')\n",
    "dp=dp.dropna()\n",
    "dp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "746e7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.drop(['id'],axis=1,inplace=True)\n",
    "Y=dp['diagnosis']\n",
    "dp.drop(['diagnosis'],axis=1,inplace=True)\n",
    "X=dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "b801d272",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y = Y.replace({'B': 1, 'M': 0})\n",
    "Y=Y.to_numpy()\n",
    "Y.shape=(Y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "6fb02b25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(377, 30) (186, 30) (377, 1) (186, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.33, shuffle=False)\n",
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "id": "5f310318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def cost_function(X,Y,w,b):\n",
    "    m=X.shape[0]\n",
    "    y_pred=sigmoid(np.dot(X,w)+b)\n",
    "    cost=(-1/m)*np.sum(Y*np.log(y_pred)+(1-Y)*np.log(1-y_pred))\n",
    "    return cost\n",
    "\n",
    "def stochastic_gradient_descent(X,Y,w,b,learning_rate,iterations):\n",
    "    m=X.shape[0]\n",
    "    cost_list=[]\n",
    "    for i in range(iterations):\n",
    "        y_pred=sigmoid(np.dot(X,w)+b)\n",
    "        w=w-(learning_rate/m)*np.dot(X.T,(y_pred-Y))\n",
    "        b=b-(learning_rate/m)*np.sum(y_pred-Y)\n",
    "        cost=cost_function(X,Y,w,b)\n",
    "        cost_list.append(cost)\n",
    "    return w,b,cost_list\n",
    "\n",
    "def predict(X,w,b):\n",
    "    y_pred=sigmoid(np.dot(X,w)+b)\n",
    "    y_pred=np.where(y_pred>0.5,1,0)\n",
    "    return y_pred\n",
    "\n",
    "def accuracy(y_pred,Y_test):\n",
    "    y_pred=y_pred.reshape(Y_test.shape)\n",
    "    return (np.sum(y_pred==Y_test)/Y_test.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "12e6934e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations is nan\n",
      "Cost after 1000 iterations is nan\n",
      "Cost after 2000 iterations is nan\n",
      "Cost after 3000 iterations is nan\n",
      "Cost after 4000 iterations is nan\n",
      "Cost after 5000 iterations is nan\n",
      "Cost after 6000 iterations is nan\n",
      "Cost after 7000 iterations is nan\n",
      "Cost after 8000 iterations is nan\n",
      "Cost after 9000 iterations is nan\n",
      "Cost after 10000 iterations is nan\n",
      "Cost after 11000 iterations is nan\n",
      "Cost after 12000 iterations is nan\n",
      "Cost after 13000 iterations is nan\n",
      "Cost after 14000 iterations is nan\n",
      "Cost after 15000 iterations is nan\n",
      "Cost after 16000 iterations is nan\n",
      "Cost after 17000 iterations is nan\n",
      "Cost after 18000 iterations is nan\n",
      "Cost after 19000 iterations is nan\n",
      "Cost after 20000 iterations is nan\n",
      "Cost after 21000 iterations is nan\n",
      "Cost after 22000 iterations is nan\n",
      "Cost after 23000 iterations is nan\n",
      "Cost after 24000 iterations is nan\n",
      "Cost after 25000 iterations is nan\n",
      "Cost after 26000 iterations is nan\n",
      "Cost after 27000 iterations is nan\n",
      "Cost after 28000 iterations is nan\n",
      "Cost after 29000 iterations is nan\n",
      "Cost after 30000 iterations is nan\n",
      "Cost after 31000 iterations is nan\n",
      "Cost after 32000 iterations is nan\n",
      "Cost after 33000 iterations is nan\n",
      "Cost after 34000 iterations is nan\n",
      "Cost after 35000 iterations is nan\n",
      "Cost after 36000 iterations is nan\n",
      "Cost after 37000 iterations is nan\n",
      "Cost after 38000 iterations is nan\n",
      "Cost after 39000 iterations is nan\n",
      "Cost after 40000 iterations is nan\n",
      "Cost after 41000 iterations is nan\n",
      "Cost after 42000 iterations is nan\n",
      "Cost after 43000 iterations is nan\n",
      "Cost after 44000 iterations is nan\n",
      "Cost after 45000 iterations is nan\n",
      "Cost after 46000 iterations is nan\n",
      "Cost after 47000 iterations is nan\n",
      "Cost after 48000 iterations is nan\n",
      "Cost after 49000 iterations is nan\n",
      "Cost after 50000 iterations is nan\n",
      "Cost after 51000 iterations is nan\n",
      "Cost after 52000 iterations is nan\n",
      "Cost after 53000 iterations is nan\n",
      "Cost after 54000 iterations is nan\n",
      "Cost after 55000 iterations is nan\n",
      "Cost after 56000 iterations is nan\n",
      "Cost after 57000 iterations is nan\n",
      "Cost after 58000 iterations is nan\n",
      "Cost after 59000 iterations is nan\n",
      "Cost after 60000 iterations is nan\n",
      "Cost after 61000 iterations is nan\n",
      "Cost after 62000 iterations is nan\n",
      "Cost after 63000 iterations is nan\n",
      "Cost after 64000 iterations is nan\n",
      "Cost after 65000 iterations is nan\n",
      "Cost after 66000 iterations is nan\n",
      "Cost after 67000 iterations is nan\n",
      "Cost after 68000 iterations is nan\n",
      "Cost after 69000 iterations is nan\n",
      "Cost after 70000 iterations is nan\n",
      "Cost after 71000 iterations is nan\n",
      "Cost after 72000 iterations is nan\n",
      "Cost after 73000 iterations is nan\n",
      "Cost after 74000 iterations is nan\n",
      "Cost after 75000 iterations is nan\n",
      "Cost after 76000 iterations is nan\n",
      "Cost after 77000 iterations is nan\n",
      "Cost after 78000 iterations is nan\n",
      "Cost after 79000 iterations is nan\n",
      "Cost after 80000 iterations is nan\n",
      "Cost after 81000 iterations is nan\n",
      "Cost after 82000 iterations is nan\n",
      "Cost after 83000 iterations is nan\n",
      "Cost after 84000 iterations is nan\n",
      "Cost after 85000 iterations is nan\n",
      "Cost after 86000 iterations is nan\n",
      "Cost after 87000 iterations is nan\n",
      "Cost after 88000 iterations is nan\n",
      "Cost after 89000 iterations is nan\n",
      "Cost after 90000 iterations is nan\n",
      "Cost after 91000 iterations is nan\n",
      "Cost after 92000 iterations is nan\n",
      "Cost after 93000 iterations is nan\n",
      "Cost after 94000 iterations is nan\n",
      "Cost after 95000 iterations is nan\n",
      "Cost after 96000 iterations is nan\n",
      "Cost after 97000 iterations is nan\n",
      "Cost after 98000 iterations is nan\n",
      "Cost after 99000 iterations is nan\n",
      "The value weights for all the features is: \n",
      "[[ 5.45552008e+00]\n",
      " [-2.30545609e+00]\n",
      " [ 1.29784539e+01]\n",
      " [ 9.57087940e-01]\n",
      " [-1.37433209e-01]\n",
      " [-5.71965232e-01]\n",
      " [-2.89607750e-01]\n",
      " [ 5.74246990e-02]\n",
      " [ 3.80480417e-01]\n",
      " [ 1.10730171e-02]\n",
      " [-3.24282649e+00]\n",
      " [-1.99832704e+01]\n",
      " [-3.62790518e+01]\n",
      " [ 2.34443907e+00]\n",
      " [-1.20777943e-01]\n",
      " [-4.41411244e-01]\n",
      " [-6.79911768e-01]\n",
      " [-2.45217477e-01]\n",
      " [-8.56868916e-02]\n",
      " [-6.75565208e-02]\n",
      " [ 1.17002064e+01]\n",
      " [-2.14398933e+00]\n",
      " [-1.37571546e+01]\n",
      " [-2.53091262e+00]\n",
      " [ 4.67495469e-02]\n",
      " [ 8.58302956e-01]\n",
      " [ 5.63310967e-01]\n",
      " [ 2.80751621e-01]\n",
      " [ 2.59983095e+00]\n",
      " [ 2.53772437e-01]]\n",
      "The value of the bias is: \n",
      "0.5207150970815351\n",
      "The accuracy of the model is: \n",
      "23.118279569892472\n"
     ]
    }
   ],
   "source": [
    "#initializing the parameters\n",
    "w=np.zeros((X_train.shape[1],1))\n",
    "b=0\n",
    "w,b,cost_list=stochastic_gradient_descent(X_train,Y_train,w,b,learning_rate=0.01,iterations=100000)\n",
    "print(\"The value weights for all the features is: \")\n",
    "print(w)\n",
    "print(\"The value of the bias is: \")\n",
    "print(b)\n",
    "y_pred=predict(X_test,w,b)\n",
    "print(\"The accuracy of the model is: \")\n",
    "print(accuracy(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9924e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value weights for all the features is: \n",
      "[[ 3.23655485e+00]\n",
      " [-3.45190695e-01]\n",
      " [ 7.73037407e+00]\n",
      " [-1.18133819e-01]\n",
      " [-4.90337373e-02]\n",
      " [-3.52023962e-01]\n",
      " [-5.10027698e-01]\n",
      " [-2.01304634e-01]\n",
      " [-7.67269762e-02]\n",
      " [-1.55251089e-02]\n",
      " [ 9.46777469e-02]\n",
      " [ 6.01652783e-01]\n",
      " [-7.59694853e-01]\n",
      " [-1.30294356e+00]\n",
      " [-6.46527196e-03]\n",
      " [-8.75772012e-02]\n",
      " [-1.17200142e-01]\n",
      " [-2.49751361e-02]\n",
      " [-2.66110385e-02]\n",
      " [-7.33436954e-03]\n",
      " [ 3.38716609e+00]\n",
      " [-4.25143241e+00]\n",
      " [-1.42886151e+00]\n",
      " [-5.41324269e-01]\n",
      " [-1.16205576e-01]\n",
      " [-1.13356467e+00]\n",
      " [-1.43725793e+00]\n",
      " [-3.72479970e-01]\n",
      " [-3.09250662e-01]\n",
      " [-9.23239527e-02]]\n",
      "The value of the bias is: \n",
      "0.4511507365687659\n",
      "The accuracy of the model is: \n",
      "92.47311827956989\n"
     ]
    }
   ],
   "source": [
    "#initializing the parameters\n",
    "w=np.zeros((X_train.shape[1],1))\n",
    "b=0\n",
    "w,b,cost_list=stochastic_gradient_descent(X_train,Y_train,w,b,learning_rate=0.001,iterations=100000)\n",
    "print(\"The value weights for all the features is: \")\n",
    "print(w)\n",
    "print(\"The value of the bias is: \")\n",
    "print(b)\n",
    "y_pred=predict(X_test,w,b)\n",
    "print(\"The accuracy of the model is: \")\n",
    "print(accuracy(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "941debed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value weights for all the features is: \n",
      "[[ 3.27641006e-01]\n",
      " [-2.62514570e-02]\n",
      " [ 7.72318815e-01]\n",
      " [-1.49762072e-02]\n",
      " [-4.91508892e-03]\n",
      " [-3.62240463e-02]\n",
      " [-5.22002703e-02]\n",
      " [-2.05007404e-02]\n",
      " [-7.89622397e-03]\n",
      " [-1.54421251e-03]\n",
      " [ 9.76902430e-03]\n",
      " [ 6.28181687e-02]\n",
      " [-7.90213742e-02]\n",
      " [-1.27140561e-01]\n",
      " [-6.51679343e-04]\n",
      " [-9.25688658e-03]\n",
      " [-1.21069629e-02]\n",
      " [-2.56498603e-03]\n",
      " [-2.59659559e-03]\n",
      " [-7.71922676e-04]\n",
      " [ 3.42229360e-01]\n",
      " [-3.90036236e-01]\n",
      " [-1.79840613e-01]\n",
      " [-4.77090478e-02]\n",
      " [-1.15853436e-02]\n",
      " [-1.19201835e-01]\n",
      " [-1.48837573e-01]\n",
      " [-3.84380191e-02]\n",
      " [-3.19848088e-02]\n",
      " [-9.72059401e-03]]\n",
      "The value of the bias is: \n",
      "0.04706959104219963\n",
      "The accuracy of the model is: \n",
      "95.6989247311828\n"
     ]
    }
   ],
   "source": [
    "#initializing the parameters\n",
    "w=np.zeros((X_train.shape[1],1))\n",
    "b=0\n",
    "w,b,cost_list=stochastic_gradient_descent(X_train,Y_train,w,b,learning_rate=0.0001,iterations=100000)\n",
    "print(\"The value weights for all the features is: \")\n",
    "print(w)\n",
    "print(\"The value of the bias is: \")\n",
    "print(b)\n",
    "y_pred=predict(X_test,w,b)\n",
    "print(\"The accuracy of the model is: \")\n",
    "print(accuracy(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "64de1908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def cost_function(X,Y,w,b):\n",
    "    m=X.shape[0]\n",
    "    y_pred=sigmoid(np.dot(X,w)+b)\n",
    "    cost=(-1/m)*np.sum(Y*np.log(y_pred)+(1-Y)*np.log(1-y_pred))\n",
    "    return cost\n",
    "\n",
    "def mini_batch_gradient_descent(X,Y,w,b,learning_rate,iterations,batch_size):\n",
    "    m=X.shape[0]\n",
    "    cost_list=[]\n",
    "    for i in range(iterations):\n",
    "        for j in range(0,m,batch_size):\n",
    "            X=pd.DataFrame(X)\n",
    "            Y=pd.DataFrame(Y)\n",
    "            X_batch=X.iloc[j:j+batch_size,:]\n",
    "            Y_batch=Y.iloc[j:j+batch_size,:]\n",
    "            X_batch=X_batch.to_numpy()\n",
    "            Y_batch=Y_batch.to_numpy()\n",
    "            y_pred=sigmoid(np.dot(X_batch,w)+b)\n",
    "            w=w-(learning_rate/m)*np.dot(X_batch.T,(y_pred-Y_batch))\n",
    "            b=b-(learning_rate/m)*np.sum(y_pred-Y_batch)\n",
    "        cost=cost_function(X,Y,w,b)\n",
    "        cost_list.append(cost)\n",
    "    return w,b,cost_list\n",
    "\n",
    "def predict(X,w,b):\n",
    "    y_pred=sigmoid(np.dot(X,w)+b)\n",
    "    y_pred=np.where(y_pred>0.5,1,0)\n",
    "    return y_pred\n",
    "\n",
    "def accuracy(y_pred,Y_test):\n",
    "    y_pred=y_pred.reshape(Y_test.shape)\n",
    "    return (np.sum(y_pred==Y_test)/Y_test.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "50343cef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value weights for all the features is: \n",
      "[[ 3.30310548e+00]\n",
      " [-1.65962117e+00]\n",
      " [ 8.21840324e+00]\n",
      " [-1.19595672e-01]\n",
      " [-4.44626931e-02]\n",
      " [-3.40001901e-01]\n",
      " [-5.08003715e-01]\n",
      " [-1.99366735e-01]\n",
      " [-7.46968271e-02]\n",
      " [-1.61119426e-02]\n",
      " [ 5.88740262e-02]\n",
      " [ 8.88125311e-01]\n",
      " [-1.15230469e+00]\n",
      " [-9.75753218e-01]\n",
      " [-5.09493350e-03]\n",
      " [-8.32483934e-02]\n",
      " [-1.11658905e-01]\n",
      " [-2.62977989e-02]\n",
      " [-2.27792372e-02]\n",
      " [-6.85426861e-03]\n",
      " [ 3.22068729e+00]\n",
      " [-2.96245049e+00]\n",
      " [-2.22310993e+00]\n",
      " [-4.05185067e-01]\n",
      " [-1.10158459e-01]\n",
      " [-1.15530582e+00]\n",
      " [-1.40878988e+00]\n",
      " [-3.84028258e-01]\n",
      " [-3.29469062e-01]\n",
      " [-9.52174900e-02]]\n",
      "The value of the bias is: \n",
      "0.4446229300566273\n",
      "The accuracy of the model is: \n",
      "91.93548387096774\n"
     ]
    }
   ],
   "source": [
    "#initializing the parameters\n",
    "w=np.zeros((X_train.shape[1],1))\n",
    "b=0\n",
    "w,b,cost_list=mini_batch_gradient_descent(X_train,Y_train,w,b,learning_rate=0.01,iterations=10000,batch_size=32)\n",
    "print(\"The value weights for all the features is: \")\n",
    "print(w)\n",
    "print(\"The value of the bias is: \")\n",
    "print(b)\n",
    "y_pred=predict(X_test,w,b)\n",
    "print(\"The accuracy of the model is: \")\n",
    "print(accuracy(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "0ab54278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value weights for all the features is: \n",
      "[[ 3.14047067e-01]\n",
      " [-1.15996495e-01]\n",
      " [ 7.57257955e-01]\n",
      " [-1.65245611e-02]\n",
      " [-4.30220773e-03]\n",
      " [-3.28865315e-02]\n",
      " [-4.80789013e-02]\n",
      " [-1.92507692e-02]\n",
      " [-7.70412411e-03]\n",
      " [-1.46464613e-03]\n",
      " [ 4.95265559e-03]\n",
      " [ 8.51985395e-02]\n",
      " [-1.13588439e-01]\n",
      " [-7.13587561e-02]\n",
      " [-5.25592898e-04]\n",
      " [-7.58126847e-03]\n",
      " [-1.00750299e-02]\n",
      " [-2.48883459e-03]\n",
      " [-2.04948800e-03]\n",
      " [-6.24443918e-04]\n",
      " [ 3.09252838e-01]\n",
      " [-2.54586382e-01]\n",
      " [-2.39357259e-01]\n",
      " [-3.01854581e-02]\n",
      " [-1.16103041e-02]\n",
      " [-1.12679162e-01]\n",
      " [-1.34041794e-01]\n",
      " [-3.74544304e-02]\n",
      " [-3.33456463e-02]\n",
      " [-9.26228223e-03]]\n",
      "The value of the bias is: \n",
      "0.043983237682981374\n",
      "The accuracy of the model is: \n",
      "91.39784946236558\n"
     ]
    }
   ],
   "source": [
    "#initializing the parameters\n",
    "w=np.zeros((X_train.shape[1],1))\n",
    "b=0\n",
    "w,b,cost_list=mini_batch_gradient_descent(X_train,Y_train,w,b,learning_rate=0.001,iterations=10000,batch_size=32)\n",
    "print(\"The value weights for all the features is: \")\n",
    "print(w)\n",
    "print(\"The value of the bias is: \")\n",
    "print(b)\n",
    "y_pred=predict(X_test,w,b)\n",
    "print(\"The accuracy of the model is: \")\n",
    "print(accuracy(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "d8e0301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value weights for all the features is: \n",
      "[[ 5.04123428e-02]\n",
      " [-3.00909196e-02]\n",
      " [ 1.69311181e-01]\n",
      " [ 6.70253810e-03]\n",
      " [-2.86654669e-04]\n",
      " [-3.88830742e-03]\n",
      " [-5.84750516e-03]\n",
      " [-2.29343611e-03]\n",
      " [-3.44644380e-04]\n",
      " [ 1.75067674e-05]\n",
      " [ 1.03202708e-03]\n",
      " [ 4.45469151e-03]\n",
      " [-1.16056286e-02]\n",
      " [-3.20672418e-02]\n",
      " [-6.64627045e-05]\n",
      " [-1.06958825e-03]\n",
      " [-1.38804525e-03]\n",
      " [-3.06220454e-04]\n",
      " [-1.88754101e-04]\n",
      " [-8.33015697e-05]\n",
      " [ 5.15385725e-02]\n",
      " [-8.79315611e-02]\n",
      " [ 3.90412425e-02]\n",
      " [-2.63967424e-02]\n",
      " [-9.04806736e-04]\n",
      " [-1.36071287e-02]\n",
      " [-1.70114966e-02]\n",
      " [-4.46739294e-03]\n",
      " [-2.85505189e-03]\n",
      " [-9.31666539e-04]]\n",
      "The value of the bias is: \n",
      "0.007812133626855591\n",
      "The accuracy of the model is: \n",
      "96.23655913978494\n"
     ]
    }
   ],
   "source": [
    "#initializing the parameters\n",
    "w=np.zeros((X_train.shape[1],1))\n",
    "b=0\n",
    "w,b,cost_list=mini_batch_gradient_descent(X_train,Y_train,w,b,learning_rate=0.0001,iterations=10000,batch_size=32)\n",
    "print(\"The value weights for all the features is: \")\n",
    "print(w)\n",
    "print(\"The value of the bias is: \")\n",
    "print(b)\n",
    "y_pred=predict(X_test,w,b)\n",
    "print(\"The accuracy of the model is: \")\n",
    "print(accuracy(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "7b936aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def cost_function(X,Y,w,b):\n",
    "    m=X.shape[0]\n",
    "    y_pred=sigmoid(np.dot(X,w)+b)\n",
    "    cost=(-1/m)*np.sum(Y*np.log(y_pred)+(1-Y)*np.log(1-y_pred))\n",
    "    return cost\n",
    "\n",
    "def batch_gradient_descent(X,Y,w,b,learning_rate,iterations,batch_size):\n",
    "    m=X.shape[0]\n",
    "    cost_list=[]\n",
    "    for i in range(iterations):\n",
    "        for j in range(0,m,batch_size):\n",
    "            X=pd.DataFrame(X)\n",
    "            Y=pd.DataFrame(Y)\n",
    "            X_batch=X.iloc[j:j+batch_size,:]\n",
    "            Y_batch=Y.iloc[j:j+batch_size,:]\n",
    "            X_batch=X_batch.to_numpy()\n",
    "            Y_batch=Y_batch.to_numpy()\n",
    "            y_pred=sigmoid(np.dot(X_batch,w)+b)\n",
    "        w=w-(learning_rate/m)*np.dot(X_batch.T,(y_pred-Y_batch))\n",
    "        b=b-(learning_rate/m)*np.sum(y_pred-Y_batch)\n",
    "        cost=cost_function(X,Y,w,b)\n",
    "        cost_list.append(cost)\n",
    "    return w,b,cost_list\n",
    "\n",
    "def predict(X,w,b):\n",
    "    y_pred=sigmoid(np.dot(X,w)+b)\n",
    "    y_pred=np.where(y_pred>0.5,1,0)\n",
    "    return y_pred\n",
    "\n",
    "def accuracy(y_pred,Y_test):\n",
    "    y_pred=y_pred.reshape(Y_test.shape)\n",
    "    return (np.sum(y_pred==Y_test)/Y_test.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "419aff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the parameters\n",
    "w=np.zeros((X_train.shape[1],1))\n",
    "b=0\n",
    "w,b,cost_list=batch_gradient_descent(X_train,Y_train,w,b,learning_rate=0.01,iterations=10000,batch_size=32)\n",
    "print(\"The value weights for all the features is: \")\n",
    "print(w)\n",
    "print(\"The value of the bias is: \")\n",
    "print(b)\n",
    "y_pred=predict(X_test,w,b)\n",
    "print(\"The accuracy of the model is: \")\n",
    "print(accuracy(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f341185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value weights for all the features is: \n",
      "[[ 6.98567740e-02]\n",
      " [-4.72541910e-02]\n",
      " [ 3.52660067e-01]\n",
      " [ 4.99999358e-02]\n",
      " [ 2.08154803e-04]\n",
      " [-1.51986947e-03]\n",
      " [-4.02359254e-03]\n",
      " [-2.35326070e-03]\n",
      " [-3.16612054e-04]\n",
      " [ 3.70182453e-04]\n",
      " [ 3.15774094e-04]\n",
      " [-2.31578581e-02]\n",
      " [-1.65099518e-02]\n",
      " [ 1.21897364e-01]\n",
      " [-1.25300055e-04]\n",
      " [-5.49885551e-04]\n",
      " [-8.18196246e-04]\n",
      " [-3.49353044e-04]\n",
      " [-2.56118167e-04]\n",
      " [-5.85057867e-05]\n",
      " [ 6.07637381e-02]\n",
      " [-1.91870500e-01]\n",
      " [ 1.95714264e-01]\n",
      " [-1.00780870e-01]\n",
      " [-7.21935034e-04]\n",
      " [-5.36486728e-03]\n",
      " [-1.01145756e-02]\n",
      " [-4.73275659e-03]\n",
      " [-1.94682449e-03]\n",
      " [-2.13752120e-04]]\n",
      "The value of the bias is: \n",
      "0.009705190449300377\n",
      "The accuracy of the model is: \n",
      "93.01075268817203\n"
     ]
    }
   ],
   "source": [
    "w=np.zeros((X_train.shape[1],1))\n",
    "b=0\n",
    "w,b,cost_list=batch_gradient_descent(X_train,Y_train,w,b,learning_rate=0.001,iterations=10000,batch_size=32)\n",
    "print(\"The value weights for all the features is: \")\n",
    "print(w)\n",
    "print(\"The value of the bias is: \")\n",
    "print(b)\n",
    "y_pred=predict(X_test,w,b)\n",
    "print(\"The accuracy of the model is: \")\n",
    "print(accuracy(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccd5794c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value weights for all the features is: \n",
      "[[ 1.08774056e-02]\n",
      " [ 2.25578350e-03]\n",
      " [ 5.93895834e-02]\n",
      " [ 1.50751007e-02]\n",
      " [ 8.01435849e-05]\n",
      " [-1.24091339e-04]\n",
      " [-4.27070164e-04]\n",
      " [-2.52098575e-04]\n",
      " [ 7.81235245e-05]\n",
      " [ 7.93177565e-05]\n",
      " [ 2.38784951e-04]\n",
      " [-1.74636269e-03]\n",
      " [-1.78913346e-04]\n",
      " [ 2.32499296e-02]\n",
      " [-7.31989334e-06]\n",
      " [-3.82524915e-05]\n",
      " [-7.69279282e-05]\n",
      " [-3.21831368e-05]\n",
      " [-7.53857040e-06]\n",
      " [-1.33111163e-06]\n",
      " [ 9.88563358e-03]\n",
      " [-1.28423105e-02]\n",
      " [ 4.36381238e-02]\n",
      " [-2.47795805e-02]\n",
      " [-9.99149024e-06]\n",
      " [-5.44964967e-04]\n",
      " [-1.12299290e-03]\n",
      " [-5.17428169e-04]\n",
      " [-4.53403553e-05]\n",
      " [ 1.55965944e-05]]\n",
      "The value of the bias is: \n",
      "0.0015735133461336947\n",
      "The accuracy of the model is: \n",
      "92.47311827956989\n"
     ]
    }
   ],
   "source": [
    "w=np.zeros((X_train.shape[1],1))\n",
    "b=0\n",
    "w,b,cost_list=batch_gradient_descent(X_train,Y_train,w,b,learning_rate=0.0001,iterations=10000,batch_size=32)\n",
    "print(\"The value weights for all the features is: \")\n",
    "print(w)\n",
    "print(\"The value of the bias is: \")\n",
    "print(b)\n",
    "y_pred=predict(X_test,w,b)\n",
    "print(\"The accuracy of the model is: \")\n",
    "print(accuracy(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ae62c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
